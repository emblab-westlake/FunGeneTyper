{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Construction of structured antibiotic resistance database (SARD) â€” Class Level Classification\n",
    "\n",
    "Here, we will provide an in-depth exposition of the construction procedure for the Structured Antibiotic Resistance\n",
    " Database (SARD), and other dataset development efforts can adhere to the procedural framework we furnish.\n",
    "\n",
    "Please ensure the following files are located in the './Tutorials/Data' directory. The download link to the data is as follows: https://drive.google.com/drive/folders/1ZM0p5YHCg2FBTQwBCHyl11L-0fzNEk_A?usp=drive_link :\n",
    "\n",
    "- **embARG-Full-V1.0-2021.7.csv**: Contains the mapping of IDs to categories in the Expanded Antibiotic Resistance Genes (ARGs) dataset.\n",
    "\n",
    "- **embARG-Full-V1.0-2021.7.fasta**: Stores IDs and their corresponding amino acid sequences in the Expanded ARGs dataset.\n",
    "\n",
    "- **uniprot-reviewed+NOT+KW-0046-filtered-full.fasta**: The Negative dataset constructed by filtering out sequences with 100% sequence identity to the ARGs dataset, sourced from the Swiss-Prot dataset.\n",
    "\n",
    "- **uniprot-reviewed+NOT+KW-0046-filtered-id80cov80.fasta**: The Negative dataset created by filtering out sequences with 80% sequence identity in comparison to the ARGs dataset, using the Swiss-Port dataset.\n",
    "\n",
    "- **uniprot-reviewed+NOT+KW-0046-filtered-id50cov80.fasta**: The Negative dataset generated by filtering out sequences with 50% sequence identity to the ARGs dataset, sourced from the Swiss-Prot dataset.\n",
    "\n",
    "- **uniprot-reviewed+NOT+KW-0046-filtered-id30cov80.fasta**: The Negative dataset formed by filtering out sequences with 30% sequence identity to the ARGs dataset, based on the Swiss-Port dataset.\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from Bio import SeqIO\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (1) Expanded ARGs dataset\n",
    "Load the Expanded Antibiotic Resistance Genes (ARGs) dataset\n",
    " and split it 8:2 into training and testing sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training numbers: 49498\n",
      "Test numbers: 12375\n",
      "{'Multi-drug resistance': 7073, 'Peptide': 655, 'Mupirocin': 115, 'MLS': 463, 'Betalactams': 1182, 'Aminoglycosides': 697, 'Rifampin': 518, 'Fosfomycin': 63, 'Tetracyclines': 410, 'Nucleosides': 52, 'Fluoroquinolones': 134, 'Glycopeptides': 407, 'Aminocoumarins': 238, 'Bacitracin': 123, 'Trimethoprim': 46, 'Phenicol': 160, 'Sulfonamide': 25, 'Fusidic acid': 4, 'Triclosan': 10}\n",
      "12375\n",
      "{'Multi-drug resistance': 28289, 'Peptide': 2621, 'Betalactams': 4727, 'MLS': 1854, 'Aminoglycosides': 2786, 'Rifampin': 2074, 'Phenicol': 640, 'Glycopeptides': 1630, 'Mupirocin': 459, 'Bacitracin': 493, 'Fluoroquinolones': 536, 'Tetracyclines': 1638, 'Trimethoprim': 182, 'Aminocoumarins': 950, 'Nucleosides': 206, 'Fosfomycin': 250, 'Sulfonamide': 103, 'Fusidic acid': 17, 'Triclosan': 43}\n",
      "49498\n",
      "{'Betalactams': 5909, 'Trimethoprim': 228, 'MLS': 2317, 'Fusidic acid': 21, 'Fosfomycin': 313, 'Aminoglycosides': 3483, 'Fluoroquinolones': 670, 'Multi-drug resistance': 35362, 'Glycopeptides': 2037, 'Phenicol': 800, 'Rifampin': 2592, 'Tetracyclines': 2048, 'Peptide': 3276, 'Bacitracin': 616, 'Sulfonamide': 128, 'Nucleosides': 258, 'Aminocoumarins': 1188, 'Triclosan': 53, 'Mupirocin': 574}\n",
      "61873\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../Tutorials\")\n",
    "Id_to_class = {}\n",
    "Id_to_class_little = {}\n",
    "with open(\"Data/embARG-Full-V1.0-2021.7.csv\") as file:\n",
    "    for i,line in enumerate(file):\n",
    "        if i == 0:\n",
    "            pass\n",
    "        else:\n",
    "            content = line.strip().split(\",\")\n",
    "            # The Elfamycins category has only one sequence and cannot be trained and verified.\n",
    "            # This category is not considered in the 19 categories\n",
    "            if content[7] != 'Elfamycins':\n",
    "                Id_to_class[content[3]] = content[7]\n",
    "                Id_to_class_little[content[3]] = content[12]\n",
    "Id_to_seq = {}\n",
    "for seq_record in SeqIO.parse(\"Data/embARG-Full-V1.0-2021.7.fasta\", \"fasta\"):\n",
    "    Id_to_seq[seq_record.id] = seq_record.seq\n",
    "\n",
    "all_data = list(Id_to_class.keys())\n",
    "all_label = list(Id_to_class.values())\n",
    "data_train,data_test,label_train,label_test = train_test_split(all_data,all_label,train_size=0.8,random_state=2021,stratify=all_label)\n",
    "\n",
    "train_number = 0\n",
    "with open(\"Data/positive_data_train.txt\",\"w\") as write:\n",
    "    for i in data_train:\n",
    "        write.write(i + \"\\t\" + str(Id_to_seq[i]) + \"\\t\" + Id_to_class[i] + \"\\t\" + Id_to_class_little[i] +\"\\n\")\n",
    "        train_number += 1\n",
    "print(f\"Training numbers: {train_number}\")\n",
    "\n",
    "test_number = 0\n",
    "with open(\"Data/positive_data_test.txt\",\"w\") as write:\n",
    "    for i in data_test:\n",
    "        write.write(i + \"\\t\" + str(Id_to_seq[i]) + \"\\t\" + Id_to_class[i] + \"\\t\" + Id_to_class_little[i] +\"\\n\")\n",
    "        test_number += 1\n",
    "print(f\"Test numbers: {test_number}\")\n",
    "\n",
    "# ouput\n",
    "category = {}\n",
    "num = 0\n",
    "for item in label_test:\n",
    "    num += 1\n",
    "    if item not in category:\n",
    "        category[item] = 1\n",
    "    else:\n",
    "        category[item] += 1\n",
    "print(category)\n",
    "print(num)\n",
    "\n",
    "category = {}\n",
    "num = 0\n",
    "for item in label_train:\n",
    "    num += 1\n",
    "    if item not in category:\n",
    "        category[item] = 1\n",
    "    else:\n",
    "        category[item] += 1\n",
    "print(category)\n",
    "print(num)\n",
    "\n",
    "category = {}\n",
    "num = 0\n",
    "for item in all_label:\n",
    "    num += 1\n",
    "    if item not in category:\n",
    "        category[item] = 1\n",
    "    else:\n",
    "        category[item] += 1\n",
    "print(category)\n",
    "print(num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (2) Negative datasets (non-ARGs datasets)\n",
    "\n",
    "1) Load four distinct negative datasets.\n",
    "\n",
    "`identity 0 (ID â‰¤0%, sequence: 453,481), identity 30 (ID â‰¤30%, sequence: 470,358),\n",
    "identity 50 (ID â‰¤50%, sequence: 474,570), and identity 80 (ID â‰¤80%, sequence: 475,049)`\n",
    "\n",
    "2) Partition the four negative datasets into training and testing sets at an 80:20 ratio, and subsequently merge\n",
    "them with the positive dataset to form the final training and testing sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/uniprot-reviewed+NOT+KW-0046-filtered-full.fasta 453481\n",
      "Data/uniprot-reviewed+NOT+KW-0046-filtered-id30cov80.fasta 470358\n",
      "Data/uniprot-reviewed+NOT+KW-0046-filtered-id50cov80.fasta 474570\n"
     ]
    }
   ],
   "source": [
    "class BigLabelClassify(object):\n",
    "    BigLabel_list = ['Betalactams', 'Trimethoprim', 'MLS', 'Fusidic acid', 'Fosfomycin', 'Aminoglycosides', 'Fluoroquinolones', 'Multi-drug resistance', 'Glycopeptides', 'Phenicol', 'Rifampin', 'Tetracyclines', 'Peptide', 'Bacitracin', 'Sulfonamide', 'Nucleosides', 'Aminocoumarins', 'Triclosan', 'Mupirocin',\"Others\"]\n",
    "\n",
    "\n",
    "def processing_negative(filename,outfile,outfilename):\n",
    "    num = 0\n",
    "    sequence_all = {}\n",
    "    try:\n",
    "        if not os.path.exists(outfile):\n",
    "            os.makedirs(outfile)\n",
    "        with open(outfile+outfilename, \"w\") as write:\n",
    "            for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "                sequence = str(seq_record.seq)\n",
    "                # Trimming the sequence to a length of 1022.\"\n",
    "                max_length = 1022\n",
    "                sequence = sequence[:max_length]\n",
    "                if sequence not in sequence_all:\n",
    "                    num += 1\n",
    "                    write.write(sequence + \"\\n\")\n",
    "                    sequence_all[sequence] = 1\n",
    "        print(filename, str(num))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{filename}' not found.\")\n",
    "\n",
    "def Split_train_test(output,input,name):\n",
    "    data_negative = []\n",
    "    with open(output+\"/\"+input,\"r\") as read:\n",
    "        for item in read:\n",
    "            data_negative.append(item.strip())\n",
    "\n",
    "    # split negative data with train and test\n",
    "    data_full_train, data_full_test = train_test_split(data_negative, train_size=0.8, random_state=2021)\n",
    "    print(output+\"/\"+input,\"train_number: \",str(len(data_full_train)),\"test_number: \",str(len(data_full_test)))\n",
    "    number = 0\n",
    "    with open(output+\"train_\"+str(name)+\".txt\",\"w\") as train:\n",
    "        for item in data_full_train:\n",
    "            number += 1\n",
    "            train.write(item + \"\\t\" + \"Others\" + \"\\t\" + \"Others\" + \"\\n\")\n",
    "        with open(\"Data/positive_data_train.txt\",\"r\") as p:\n",
    "            for item in p:\n",
    "                _,seq,category_big,category_little = item.strip().split(\"\\t\")\n",
    "                train.write(seq + \"\\t\" + category_big +\"\\t\"+ category_little +\"\\n\")\n",
    "                number += 1\n",
    "    print(f\"all_train {number}\")\n",
    "    number = 0\n",
    "    with open(output+\"test_\"+str(name)+\".txt\", \"w\") as train:\n",
    "        for item in data_full_test:\n",
    "            number += 1\n",
    "            train.write(item + \"\\t\" + \"Others\" + \"\\t\" + \"Others\" + \"\\n\")\n",
    "        with open(\"Data/positive_data_test.txt\", \"r\") as p:\n",
    "            for item in p:\n",
    "                _, seq, category_big, category_little = item.strip().split(\"\\t\")\n",
    "                train.write(seq + \"\\t\" + category_big + \"\\t\" + category_little + \"\\n\")\n",
    "                number += 1\n",
    "    print(f\"all_test {number}\")\n",
    "\n",
    "\n",
    "def processingTestData(dir,file_name):\n",
    "    with open(dir+\"finally_test.txt\",\"w\") as write:\n",
    "        with open(dir+file_name,\"r\") as read:\n",
    "            for item in read:\n",
    "                seq, label_big, label_small = item.strip().split(\"\\t\")\n",
    "                write.write(seq + \"\\t\" + str(BigLabelClassify.BigLabel_list.index(label_big)) + \"\\n\")\n",
    "    test_category = {}\n",
    "    with open(dir+\"finally_test.txt\",\"r\") as read:\n",
    "        for item in read:\n",
    "            _,category = item.strip().split(\"\\t\")\n",
    "            cate = int(category)\n",
    "            if cate not in test_category:\n",
    "                test_category[cate] = 1\n",
    "            else:\n",
    "                test_category[cate] += 1\n",
    "    print(test_category)\n",
    "\n",
    "processing_negative(\"Data/uniprot-reviewed+NOT+KW-0046-filtered-full.fasta\",\"Data/full/\",\"full_negative.txt\")\n",
    "processing_negative(\"Data/uniprot-reviewed+NOT+KW-0046-filtered-id30cov80.fasta\",\"Data/30/\",\"30_negative.txt\")\n",
    "processing_negative(\"Data/uniprot-reviewed+NOT+KW-0046-filtered-id50cov80.fasta\",\"Data/50/\",\"50_negative.txt\")\n",
    "processing_negative(\"Data/uniprot-reviewed+NOT+KW-0046-filtered-id80cov80.fasta\",\"Data/80/\",\"80_negative.txt\")\n",
    "\n",
    "\n",
    "Split_train_test(\"Data/30/\",\"30_negative.txt\",30)\n",
    "Split_train_test(\"Data/50/\",\"50_negative.txt\", 50)\n",
    "Split_train_test(\"Data/80/\",\"80_negative.txt\", 80)\n",
    "Split_train_test(\"Data/full/\",\"full_negative.txt\", \"full\")\n",
    "\n",
    "\n",
    "processingTestData(\"Data/30/\",\"test_30.txt\")\n",
    "processingTestData(\"Data/50/\",\"test_50.txt\")\n",
    "processingTestData(\"Data/80/\",\"test_80.txt\")\n",
    "processingTestData(\"Data/full/\",\"test_full.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (4) Conduct 5-fold cross-validation on the training dataset and implement oversampling to maintain balanced category distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data length: 412282, Using 5 Fold to process data... \n"
     ]
    }
   ],
   "source": [
    "def KFoldProcessing(dir_name:Union[str,Path]):\n",
    "    if not os.path.exists(dir_name):\n",
    "        raise ValueError(\"dir_name is not correct! please confirm the correct dir_name!\")\n",
    "    all_data = np.loadtxt(dir_name, delimiter='\\t', dtype=list)\n",
    "    sequence = all_data[:,0]\n",
    "    labelBig = all_data[:,1]\n",
    "    labelSmall = all_data[:,2]\n",
    "\n",
    "    # big label -> index\n",
    "    Index_BigLabel = [BigLabelClassify.BigLabel_list.index(x) for x in labelBig]\n",
    "    print(\"All Data length: {}, Using {} Fold to process data... \".format(len(all_data),5))\n",
    "\n",
    "    # 5-fold\n",
    "    KfoldData = StratifiedKFold(n_splits=5,random_state=2021,shuffle=True)\n",
    "    Data_Fold = {}\n",
    "    folder_name = \"/\".join(dir_name.split(\"/\")[:-1])\n",
    "    for index,(train_index,test_index) in enumerate(KfoldData.split(sequence,Index_BigLabel)):\n",
    "        Data_Fold[index] = {\"train\":{\n",
    "                                     \"sequence\": sequence[train_index], \"label\": np.array(Index_BigLabel)[train_index]},\n",
    "                             \"vail\":{\n",
    "                                     \"sequence\": sequence[test_index], \"label\": np.array(Index_BigLabel)[test_index]}\n",
    "                            }\n",
    "\n",
    "        with open(folder_name + \"/train_padding_\" + str(index + 1) + \".txt\", \"w\") as write:\n",
    "            for seq_write, label_write in zip(Data_Fold[index][\"train\"][\"sequence\"],\n",
    "                                              Data_Fold[index][\"train\"][\"label\"]):\n",
    "                write.write(seq_write + \"\\t\" + str(label_write) + \"\\n\")\n",
    "        with open(folder_name + \"/train_\" + str(index + 1) + \".txt\", \"w\") as write:\n",
    "            for seq_write, label_write in zip(Data_Fold[index][\"train\"][\"sequence\"],\n",
    "                                              Data_Fold[index][\"train\"][\"label\"]):\n",
    "                write.write(seq_write + \"\\t\" + str(label_write) + \"\\n\")\n",
    "        with open(folder_name + \"/Vail_\" + str(index + 1) + \".txt\", \"w\") as write:\n",
    "            for seq_write, label_write in zip(Data_Fold[index][\"vail\"][\"sequence\"],\n",
    "                                              Data_Fold[index][\"vail\"][\"label\"]):\n",
    "                write.write(seq_write + \"\\t\" + str(label_write) + \"\\n\")\n",
    "\n",
    "    \"\"\"\n",
    "    Balancing the data only for the positive category\n",
    "    \"\"\"\n",
    "    MAX = {}\n",
    "    Category_number = {}\n",
    "    for item in Data_Fold:\n",
    "        train_data_label = Data_Fold[item][\"train\"][\"label\"]\n",
    "        category = {}\n",
    "        for label in train_data_label:\n",
    "            if label not in category:\n",
    "                category[label] = 1\n",
    "            else:\n",
    "                category[label] += 1\n",
    "        # MAX.append(sorted(category.values(),reverse=True)[1])\n",
    "        del category[19]\n",
    "        MAX[item] = max(category.values())\n",
    "        Category_number[item] = category\n",
    "    print(\"Balance data for positive data...\")\n",
    "    for item in Data_Fold:\n",
    "        max_fold = MAX[item]\n",
    "        origin_category = Category_number[item]\n",
    "        sequence_fold = Data_Fold[item][\"train\"][\"sequence\"]\n",
    "        label_fold = Data_Fold[item][\"train\"][\"label\"]\n",
    "\n",
    "        num_padding = {}\n",
    "        zero_num_padding = {}\n",
    "        for i in origin_category:\n",
    "            num_padding[i] = max_fold - origin_category[i]\n",
    "            zero_num_padding[i] = 0\n",
    "\n",
    "        with open(folder_name + \"/train_padding_\" + str(item + 1) + \".txt\", \"a+\") as write:\n",
    "            while zero_num_padding != num_padding:\n",
    "                for seq,label_big in zip(sequence_fold,label_fold):\n",
    "                    if label_big != 19 and zero_num_padding[label_big] < num_padding[label_big]:\n",
    "                        write.write(seq +\"\\t\"+str(label_big)+\"\\n\")\n",
    "                        zero_num_padding[label_big] += 1\n",
    "        balance_category = {}\n",
    "        with open(folder_name + \"/train_padding_\" + str(item + 1) + \".txt\", \"r\") as read:\n",
    "            for item in read:\n",
    "                _,category = item.strip().split(\"\\t\")\n",
    "                if int(category) not in balance_category:\n",
    "                    balance_category[int(category)] = 1\n",
    "                else:\n",
    "                    balance_category[int(category)] += 1\n",
    "    gc.collect()\n",
    "\n",
    "'''\n",
    "KFoldProcessing(\"Data/30/train_30.txt\")\n",
    "KFoldProcessing(\"Data/50/train_50.txt\")\n",
    "KFoldProcessing(\"Data/80/train_80.txt\")\n",
    "'''\n",
    "KFoldProcessing(\"Data/full/train_full.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ðŸŽ‰ Now that you have all the necessary data prepared for training, please refer to the 'FunGeneTyper/README.md' for guidance on model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## (5) Constructing the final dataset using the 'uniprot-reviewed+NOT+KW-0046-filtered-full.fasta' as the negative dataset.\n",
    "\n",
    "\n",
    "After validation, it was determined that 'uniprot-reviewed+NOT+KW-0046-filtered-full.fasta' is the most effective negative dataset for training. Consequently,\n",
    " the 'uniprot-reviewed+NOT+KW-0046-filtered-full.fasta' dataset is processed and divided into training, validation, and test sets in a 6:2:2 ratio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def SplitTrainToVail(dir_train:Union[str,Path],dir_test:Union[str,Path],):\n",
    "    filename_write = \"Data/ARGs_ClassLevel\"\n",
    "    if not os.path.exists(filename_write):\n",
    "        os.makedirs(filename_write)\n",
    "    all_data = np.loadtxt(dir_train, delimiter='\\t', dtype=list)\n",
    "    sequence = all_data[:,0]\n",
    "    labelBig = all_data[:,1]\n",
    "    labelSmall = all_data[:,2]\n",
    "\n",
    "    # label -> index\n",
    "    Index_BigLabel = [BigLabelClassify.BigLabel_list.index(x) for x in labelBig]\n",
    "    print(\"All Data length: {}, Split data to train and Vaildation. \".format(len(all_data)))\n",
    "\n",
    "    data_train, data_test, label_train, label_test = train_test_split(sequence, Index_BigLabel, train_size=0.75,\n",
    "                                                                      random_state=2021, stratify=Index_BigLabel)\n",
    "    data_train = list(data_train)\n",
    "    data_test = list(data_test)\n",
    "    label_train = list(label_train)\n",
    "    label_test = list(label_test)\n",
    "    with open(filename_write+\"/\"+\"Train_full.txt\",\"w\") as write:\n",
    "        for seq,categogy in zip(data_train,label_train):\n",
    "            write.write(seq +\"\\t\"+ str(categogy) +\"\\n\")\n",
    "    with open(filename_write+\"/\"+\"TrainPaddingAll_full.txt\",\"w\") as write:\n",
    "        for seq,categogy in zip(data_train,label_train):\n",
    "            write.write(seq +\"\\t\"+ str(categogy) +\"\\n\")\n",
    "    with open(filename_write+\"/\"+\"Vail_full.txt\", \"w\") as write:\n",
    "        for seq, categogy in zip(data_test, label_test):\n",
    "            write.write(seq + \"\\t\" + str(categogy) + \"\\n\")\n",
    "    with open(filename_write+\"/\"+\"Test_full.txt\", \"w\") as write:\n",
    "        with open(dir_test,\"r\") as read:\n",
    "            for item in read:\n",
    "                seq,big,little = item.strip().split(\"\\t\")\n",
    "                write.write(seq +\"\\t\"+ str(BigLabelClassify.BigLabel_list.index(big)) +\"\\n\")\n",
    "    out = {}\n",
    "    with open(filename_write+\"/\"+\"Train_full.txt\", \"r\") as read:\n",
    "        for item in read:\n",
    "            seq_, category_ = item.strip().split(\"\\t\")\n",
    "            if int(category_) not in out:\n",
    "                out[int(category_)] = 1\n",
    "            else:\n",
    "                out[int(category_)] += 1\n",
    "    print(out)\n",
    "    MAX = max(out.values())\n",
    "    del out[19]\n",
    "    Category_positive = out\n",
    "    num_padding = {}\n",
    "    zero_num_padding = {}\n",
    "    for i in Category_positive:\n",
    "        num_padding[i] = MAX - Category_positive[i]\n",
    "        zero_num_padding[i] = 0\n",
    "    with open(filename_write+\"/\"+\"TrainPaddingAll_full.txt\", \"a+\") as write:\n",
    "        while zero_num_padding != num_padding:\n",
    "            for seq, label_big in zip(data_train, label_train):\n",
    "                if label_big != 19 and zero_num_padding[label_big] < num_padding[label_big]:\n",
    "                    write.write(seq + \"\\t\" + str(label_big) + \"\\n\")\n",
    "                    zero_num_padding[label_big] += 1\n",
    "                    print(zero_num_padding)\n",
    "    \"\"\"\n",
    "    just output and see\n",
    "    \"\"\"\n",
    "    ##########################\n",
    "    out = {}\n",
    "    with open(filename_write+\"/\"+\"Train_full.txt\", \"r\") as read:\n",
    "        for item in read:\n",
    "            seq_,category_ = item.strip().split(\"\\t\")\n",
    "            if int(category_) not in out:\n",
    "                out[int(category_)] = 1\n",
    "            else:\n",
    "                out[int(category_)] += 1\n",
    "    print(out)\n",
    "    out = {}\n",
    "    with open(filename_write+\"/\"+\"TrainPaddingAll_full.txt\", \"r\") as read:\n",
    "        for item in read:\n",
    "            seq_, category_ = item.strip().split(\"\\t\")\n",
    "            if int(category_) not in out:\n",
    "                out[int(category_)] = 1\n",
    "            else:\n",
    "                out[int(category_)] += 1\n",
    "    print(out)\n",
    "    out = {}\n",
    "    with open(filename_write+\"/\"+\"Vail_full.txt\", \"r\") as read:\n",
    "        for item in read:\n",
    "            seq_,category_ = item.strip().split(\"\\t\")\n",
    "            if int(category_) not in out:\n",
    "                out[int(category_)] = 1\n",
    "            else:\n",
    "                out[int(category_)] += 1\n",
    "    print(out)\n",
    "    out = {}\n",
    "    with open(filename_write+\"/\"+\"Test_full.txt\", \"r\") as read:\n",
    "        for item in read:\n",
    "            seq_, category_ = item.strip().split(\"\\t\")\n",
    "            if int(category_) not in out:\n",
    "                out[int(category_)] = 1\n",
    "            else:\n",
    "                out[int(category_)] += 1\n",
    "    print(out)\n",
    "\n",
    "SplitTrainToVail(\"Data/full/train_full.txt\",\"Data/full/test_full.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ðŸ‘ You can employ the training, validation, and test sets to replicate ARGTyper results of Class Level presented in our paper.\n",
    "Furthermore, we also offer a direct download link to these datasets: https://drive.google.com/drive/folders/1uKP9-IIkOXqgQYSSfruycdCyl0otY41J?usp=drive_link\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}